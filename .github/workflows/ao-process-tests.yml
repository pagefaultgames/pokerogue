# GitHub Actions Workflow for AO Process Testing
# Automated testing pipeline for development environment and parity validation

name: AO Process Test Suite

on:
  push:
    branches: [ main, beta, develop ]
    paths:
      - 'ao-processes/**'
      - 'parity-testing/**'
      - 'development-tools/**'
      - '.github/workflows/ao-process-tests.yml'
  pull_request:
    branches: [ main, beta ]
    paths:
      - 'ao-processes/**'
      - 'parity-testing/**'
      - 'development-tools/**'
  schedule:
    # Run tests daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_level:
        description: 'Test level to run'
        required: false
        default: 'full'
        type: choice
        options:
          - 'unit'
          - 'integration'
          - 'parity'
          - 'performance' 
          - 'full'
      skip_parity:
        description: 'Skip parity tests (faster execution)'
        required: false
        default: false
        type: boolean
      parallel_jobs:
        description: 'Number of parallel test jobs'
        required: false
        default: '4'
        type: string

env:
  LUA_VERSION: '5.3'
  NODE_VERSION: '18'
  TEST_TIMEOUT: '1800' # 30 minutes
  ARTIFACT_RETENTION_DAYS: '30'

jobs:
  # Job 1: Test Environment Setup and Validation
  setup:
    name: Setup Test Environment
    runs-on: ubuntu-latest
    outputs:
      lua-version: ${{ steps.set-lua-version.outputs.lua-version }}
      test-strategy: ${{ steps.determine-strategy.outputs.strategy }}
      ao-ready: ${{ steps.check-ao-ready.outputs.ready }}
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 # Full history for trend analysis
          submodules: recursive # Include aolite submodule
      
      - name: Check AO Process Readiness
        id: check-ao-ready
        run: |
          echo "ğŸ” Checking if AO processes are ready for testing..."
          
          # Check if required directories and files exist for aolite-based testing
          if [ ! -d "ao-processes" ] || \
             [ ! -f "ao-processes/main.lua" ] || \
             [ ! -d "ao-processes/tests" ]; then
            echo "âŒ Required AO process directories not found"
            echo "ready=false" >> $GITHUB_OUTPUT
            echo "::warning::AO Process tests skipped - basic structure not ready"
            exit 0
          fi
          
          echo "âœ… AO processes ready for aolite testing"
          echo "ready=true" >> $GITHUB_OUTPUT
      
      - name: Setup Lua Environment
        id: setup-lua
        uses: leafo/gh-actions-lua@v10
        with:
          luaVersion: "5.3"
      
      - name: Set Lua Version Output
        id: set-lua-version
        run: |
          echo "lua-version=${{ env.LUA_VERSION }}" >> $GITHUB_OUTPUT
      
      - name: Setup LuaRocks
        uses: leafo/gh-actions-luarocks@v4
        
      - name: Cache Lua Dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.luarocks
            ./lua_modules
          key: lua-deps-${{ runner.os }}-${{ env.LUA_VERSION }}-${{ hashFiles('**/rockspec') }}
          restore-keys: |
            lua-deps-${{ runner.os }}-${{ env.LUA_VERSION }}-
            lua-deps-${{ runner.os }}-
      
      - name: Setup Aolite from Submodule
        run: |
          echo "ğŸ“¦ Setting up aolite from git submodule..."
          
          # Verify aolite submodule is available
          if [ ! -d "development-tools/aolite" ]; then
            echo "âŒ Aolite submodule not found - ensure submodules were checked out"
            exit 1
          fi
          
          echo "âœ… Aolite submodule available"
          
          # List contents to debug
          echo "ğŸ“‚ Aolite directory contents:"
          ls -la development-tools/aolite/
          
          # Try to install aolite using luarocks
          cd development-tools/aolite
          
          # Install aolite locally
          if command -v luarocks >/dev/null 2>&1; then
            echo "Installing aolite via luarocks..."
            luarocks make || echo "âš ï¸ Luarocks install failed, will try direct usage"
          else
            echo "âš ï¸ Luarocks not available, using direct file access"
          fi
          
          # Test basic aolite loading
          echo "ğŸ§ª Testing aolite loading..."
          
          # Determine which lua command to use and verify version
          if command -v lua >/dev/null 2>&1; then
            LUA_CMD="lua"
            LUA_VERSION_CHECK=$(lua -v 2>&1 | head -1)
            echo "Found lua command: $LUA_VERSION_CHECK"
            if [[ ! "$LUA_VERSION_CHECK" =~ "Lua 5.3" ]]; then
              echo "âš ï¸ Warning: Expected Lua 5.3 but found: $LUA_VERSION_CHECK"
            fi
          elif command -v lua5.3 >/dev/null 2>&1; then
            LUA_CMD="lua5.3"
            echo "Found lua5.3 command"
          else
            echo "âŒ No Lua interpreter found"
            exit 1
          fi
          
          $LUA_CMD -e "
            -- Try to load aolite from lua/ directory
            package.path = package.path .. ';./lua/?.lua;./lua/aolite/?.lua'
            
            local success, aolite = pcall(require, 'aolite.main')
            if not success then
              success, aolite = pcall(require, 'aolite')
            end
            if not success then
              success, aolite = pcall(require, 'main')
            end
            
            if success then
              print('âœ… Aolite loaded successfully')
            else
              print('âš ï¸ Aolite loading failed, will handle gracefully in tests')
            end
          " || echo "âš ï¸ Aolite test failed, continuing with graceful handling"
          
          cd ../..
      
      - name: Determine Test Strategy
        id: determine-strategy
        run: |
          if [ "${{ github.event_name }}" == "schedule" ]; then
            echo "strategy=comprehensive" >> $GITHUB_OUTPUT
          elif [ "${{ github.event_name }}" == "pull_request" ]; then
            echo "strategy=targeted" >> $GITHUB_OUTPUT
          else
            echo "strategy=standard" >> $GITHUB_OUTPUT
          fi
      
      - name: Validate Test Environment
        run: |
          echo "ğŸ” Validating aolite test environment..."
          
          # Check which lua command is available
          if command -v lua5.3 >/dev/null 2>&1; then
            LUA_CMD="lua5.3"
          elif command -v lua >/dev/null 2>&1; then
            LUA_CMD="lua"
          else
            echo "âŒ No Lua interpreter found"
            exit 1
          fi
          
          echo "Using Lua command: $LUA_CMD"
          $LUA_CMD -v
          echo "âœ… Lua environment ready"
          
          # Test aolite integration with our AO processes
          cd development-tools/aolite
          $LUA_CMD -e "
            -- Try different paths to find aolite
            package.path = package.path .. ';./lua/?.lua;./lua/aolite/?.lua;./?.lua;../../?.lua'
            
            local success, aolite = pcall(require, 'aolite.main')
            if not success then
              success, aolite = pcall(require, 'aolite')
            end
            if not success then
              success, aolite = pcall(require, 'main')
            end
            
            if success then
              print('âœ… Aolite loaded, testing AO process integration...')
              
              -- Test spawning our main AO process
              local process_success, process_result = pcall(function()
                local process_id = aolite.spawnProcess({
                  file = '../../ao-processes/main.lua',
                  name = 'test-main'
                })
                return process_id
              end)
              
              if process_success then
                print('âœ… AO process spawning works')
                if type(aolite.resetAll) == 'function' then
                  aolite.resetAll()
                end
              else
                print('âš ï¸ AO process spawning failed:', process_result)
              end
              
              print('âœ… Aolite integration test completed')
            else
              print('âš ï¸ Aolite not available, tests will handle gracefully')
            end
          " || echo "âš ï¸ Aolite validation failed, tests will handle gracefully"
          cd ../..
          
          echo "âœ… Test environment validated"

  # Job 2: Unit Tests
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    needs: setup
    if: >
      needs.setup.outputs.ao-ready == 'true' && (
      github.event.inputs.test_level == 'unit' || 
      github.event.inputs.test_level == 'full' ||
      github.event.inputs.test_level == '' ||
      github.event_name != 'workflow_dispatch')
    
    strategy:
      matrix:
        test-group: [1, 2, 3, 4]
      fail-fast: false
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          submodules: recursive # Include aolite submodule
      
      - name: Setup Lua
        uses: leafo/gh-actions-lua@v10
        with:
          luaVersion: ${{ needs.setup.outputs.lua-version }}
      
      - name: Restore Dependencies Cache
        uses: actions/cache@v4
        with:
          path: |
            ~/.luarocks
            ./lua_modules
          key: lua-deps-${{ runner.os }}-${{ env.LUA_VERSION }}-${{ hashFiles('**/rockspec') }}
      
      - name: Run Unit Tests with Aolite
        id: unit-tests
        timeout-minutes: 15
        run: |
          echo "ğŸ§ª Running Unit Tests with Aolite (Group ${{ matrix.test-group }})"
          
          # Set test group environment variable
          export TEST_GROUP=${{ matrix.test-group }}
          export MAX_PARALLEL_TESTS=${{ github.event.inputs.parallel_jobs || '4' }}
          
          # Create aolite test output directory
          mkdir -p test-results/aolite-unit-tests
          
          # Run our working AO process unit tests directly
          cd ao-processes/tests/unit
          
          # Determine which lua command to use and verify version
          if command -v lua >/dev/null 2>&1; then
            LUA_CMD="lua"
            LUA_VERSION_CHECK=$(lua -v 2>&1 | head -1)
            echo "Found lua command: $LUA_VERSION_CHECK"
            if [[ ! "$LUA_VERSION_CHECK" =~ "Lua 5.3" ]]; then
              echo "âš ï¸ Warning: Expected Lua 5.3 but found: $LUA_VERSION_CHECK"
            fi
          elif command -v lua5.3 >/dev/null 2>&1; then
            LUA_CMD="lua5.3"
            echo "Found lua5.3 command"
          else
            echo "âŒ No Lua interpreter found"
            exit 1
          fi
          
          echo "ğŸ§ª Running AO Process Unit Tests (Group ${{ matrix.test-group }})"
          
          # Define test groups
          case ${{ matrix.test-group }} in
            1)
              TESTS="main.test.lua handler-framework.test.lua"
              ;;
            2)
              TESTS="admin-handler.test.lua error-handling.test.lua"
              ;;
            3)
              TESTS="auth-handler.test.lua validation-handler.test.lua"
              ;;
            4)
              TESTS="anti-cheat-handler.test.lua enhanced-test-framework.test.lua"
              ;;
            *)
              TESTS="main.test.lua"
              ;;
          esac
          
          # Run each test in the group
          TOTAL_FAILURES=0
          TESTS_RUN=0
          for TEST_FILE in $TESTS; do
            if [ -f "$TEST_FILE" ]; then
              echo "ğŸ§ª Running $TEST_FILE..."
              TESTS_RUN=$((TESTS_RUN + 1))
              if $LUA_CMD "$TEST_FILE"; then
                echo "âœ… $TEST_FILE passed"
              else
                echo "âŒ $TEST_FILE failed"
                TOTAL_FAILURES=$((TOTAL_FAILURES + 1))
              fi
            else
              echo "âš ï¸ $TEST_FILE not found, skipping"
            fi
          done
          
          # If no tests were found, try running any available test files
          if [ $TESTS_RUN -eq 0 ]; then
            echo "âš ï¸ No specified tests found, running available tests..."
            for TEST_FILE in *.test.lua; do
              if [ -f "$TEST_FILE" ]; then
                echo "ğŸ§ª Running $TEST_FILE..."
                if $LUA_CMD "$TEST_FILE"; then
                  echo "âœ… $TEST_FILE passed"
                else
                  echo "âŒ $TEST_FILE failed"
                  TOTAL_FAILURES=$((TOTAL_FAILURES + 1))
                fi
                break  # Run at least one test to validate setup
              fi
            done
          fi
          
          # Write results to test-results directory
          cd ../../../
          mkdir -p test-results/aolite-unit-tests
          echo "AO Process Unit Tests Group ${{ matrix.test-group }}" > test-results/aolite-unit-tests/results.log
          echo "Total failures: $TOTAL_FAILURES" >> test-results/aolite-unit-tests/results.log
          
          if [ $TOTAL_FAILURES -gt 0 ]; then
            echo "âŒ $TOTAL_FAILURES tests failed in group ${{ matrix.test-group }}"
            exit 1
          else
            echo "âœ… All tests passed in group ${{ matrix.test-group }}"
          fi
      
      - name: Upload Unit Test Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: unit-test-results-group-${{ matrix.test-group }}
          path: |
            test-results/
            *.log
          retention-days: ${{ env.ARTIFACT_RETENTION_DAYS }}
      
      - name: Comment Test Results on PR
        if: github.event_name == 'pull_request' && (success() || failure())
        uses: actions/github-script@v7
        with:
          script: |
            const testGroup = '${{ matrix.test-group }}';
            const success = '${{ steps.unit-tests.outcome }}' === 'success';
            const emoji = success ? 'âœ…' : 'âŒ';
            const status = success ? 'PASSED' : 'FAILED';
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `${emoji} Unit Tests Group ${testGroup}: ${status}`
            });

  # Job 3: Integration Tests (DISABLED - Foundation still being built)
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: [setup, unit-tests]
    if: false  # Disabled until AO foundation is complete
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
      
      - name: Setup Lua
        uses: leafo/gh-actions-lua@v10
        with:
          luaVersion: ${{ needs.setup.outputs.lua-version }}
      
      - name: Setup Node.js (for TypeScript comparison)
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
      
      - name: Install TypeScript Dependencies
        run: |
          if [ -f "package.json" ]; then
            npm install --only=dev
          fi
      
      - name: Run Integration Tests  
        id: integration-tests
        timeout-minutes: 20
        run: |
          echo "ğŸ”— Running AO Process Integration Tests"
          
          # Create test output directory
          mkdir -p test-results/aolite-integration-tests
          
          cd ao-processes/tests/integration
          
          # Determine which lua command to use and verify version
          if command -v lua >/dev/null 2>&1; then
            LUA_CMD="lua"
            LUA_VERSION_CHECK=$(lua -v 2>&1 | head -1)
            echo "Found lua command: $LUA_VERSION_CHECK"
            if [[ ! "$LUA_VERSION_CHECK" =~ "Lua 5.3" ]]; then
              echo "âš ï¸ Warning: Expected Lua 5.3 but found: $LUA_VERSION_CHECK"
            fi
          elif command -v lua5.3 >/dev/null 2>&1; then
            LUA_CMD="lua5.3"
            echo "Found lua5.3 command"
          else
            echo "âŒ No Lua interpreter found"
            exit 1
          fi
          
          # Run integration tests if they exist
          TOTAL_FAILURES=0
          for TEST_FILE in *.test.lua; do
            if [ -f "$TEST_FILE" ]; then
              echo "ğŸ”— Running $TEST_FILE..."
              if $LUA_CMD "$TEST_FILE"; then
                echo "âœ… $TEST_FILE passed"
              else
                echo "âŒ $TEST_FILE failed"
                TOTAL_FAILURES=$((TOTAL_FAILURES + 1))
              fi
            else
              echo "âš ï¸ No integration tests found, running basic validation"
              echo "âœ… Integration validation completed"
              break
            fi
          done
          
          cd ../../../
          echo "Integration Tests Completed" > test-results/aolite-integration-tests/results.log
          echo "Total failures: $TOTAL_FAILURES" >> test-results/aolite-integration-tests/results.log
          
          if [ $TOTAL_FAILURES -gt 0 ]; then
            echo "âŒ $TOTAL_FAILURES integration tests failed"
            exit 1
          else
            echo "âœ… All integration tests passed"
          fi
      
      - name: Upload Integration Test Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: integration-test-results
          path: |
            test-results/
            *.log
            *.html
          retention-days: ${{ env.ARTIFACT_RETENTION_DAYS }}

  # Job 4: Parity Tests (DISABLED - No TypeScript reference implementation yet)
  parity-tests:
    name: Parity Validation Tests
    runs-on: ubuntu-latest
    needs: [setup, unit-tests]
    if: false  # Disabled until TypeScript->Lua migration is ready
    
    strategy:
      matrix:
        parity-suite: ['pokemon-stats', 'battle-damage', 'type-effectiveness']
      fail-fast: false
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
      
      - name: Setup Lua
        uses: leafo/gh-actions-lua@v10
        with:
          luaVersion: ${{ needs.setup.outputs.lua-version }}
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
      
      - name: Install Dependencies
        run: |
          if [ -f "package.json" ]; then
            npm install
          fi
      
      - name: Run Parity Tests
        id: parity-tests
        timeout-minutes: 30
        run: |
          echo "âš–ï¸ Running Parity Tests: ${{ matrix.parity-suite }}"
          
          # Determine which lua command to use and verify version
          if command -v lua >/dev/null 2>&1; then
            LUA_CMD="lua"
            LUA_VERSION_CHECK=$(lua -v 2>&1 | head -1)
            echo "Found lua command: $LUA_VERSION_CHECK"
            if [[ ! "$LUA_VERSION_CHECK" =~ "Lua 5.3" ]]; then
              echo "âš ï¸ Warning: Expected Lua 5.3 but found: $LUA_VERSION_CHECK"
            fi
          elif command -v lua5.3 >/dev/null 2>&1; then
            LUA_CMD="lua5.3"
            echo "Found lua5.3 command"
          else
            echo "âŒ No Lua interpreter found"
            exit 1
          fi
          
          $LUA_CMD -e "
            package.path = package.path .. ';./?.lua;./ao-processes/?.lua;./parity-testing/?.lua'
            
            -- Try to load test runner, skip if not available
            local success, AdvancedTestRunner = pcall(require, 'ao-processes.tests.advanced-test-runner')
            if not success then
              print('âš ï¸ Advanced test runner not available, skipping parity tests')
              os.exit(0)
            end
            
            AdvancedTestRunner.configure({
              max_parallel_tests = 2,
              test_timeout = 120000, -- 2 minute timeout for parity tests
              enable_parity_testing = true,
              generate_comprehensive_reports = true,
              report_formats = { 'text', 'json', 'html' }
            })
            
            local results = AdvancedTestRunner.runTestSuite({
              skip_unit_tests = true,
              skip_integration_tests = true,
              skip_performance_tests = true,
              parity_suite_filter = '${{ matrix.parity-suite }}'
            })
            
            if not results.overall_success then
              print('âš ï¸ Parity tests failed - this may indicate behavioral differences between TypeScript and Lua implementations')
              os.exit(1)
            end
          "
      
      - name: Generate Parity Report
        if: always()
        run: |
          echo "ğŸ“Š Generating comprehensive parity report..."
          
          lua -e "
            package.path = package.path .. ';./?.lua;./parity-testing/?.lua'
            local ParityReporter = require('parity-testing.reports.parity-reporter')
            
            -- Generate mock results for demonstration
            local mock_results = {
              ['${{ matrix.parity-suite }}'] = {
                validation_count = 10,
                passed_count = 9,
                failed_count = 1,
                parity_rate = 0.9,
                critical_failures = 0,
                execution_time = 15000
              }
            }
            
            local report = ParityReporter.generateComprehensiveReport(mock_results, {
              format = 'html'
            })
            
            local file = io.open('parity-report-${{ matrix.parity-suite }}.html', 'w')
            if file then
              file:write(report)
              file:close()
              print('ğŸ“„ Parity report generated: parity-report-${{ matrix.parity-suite }}.html')
            end
          "
      
      - name: Upload Parity Test Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: parity-test-results-${{ matrix.parity-suite }}
          path: |
            test-results/
            parity-report-*.html
            *.log
          retention-days: ${{ env.ARTIFACT_RETENTION_DAYS }}

  # Job 5: Performance Tests (DISABLED - Premature for foundation phase)
  performance-tests:
    name: Performance & Benchmarking
    runs-on: ubuntu-latest
    needs: [setup, unit-tests]
    if: false  # Disabled until basic functionality is complete
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
      
      - name: Setup Lua
        uses: leafo/gh-actions-lua@v10
        with:
          luaVersion: ${{ needs.setup.outputs.lua-version }}
      
      - name: Run Performance Tests
        id: performance-tests
        timeout-minutes: 25
        run: |
          echo "ğŸƒ Running Performance Tests and Benchmarks"
          
          # Determine which lua command to use and verify version
          if command -v lua >/dev/null 2>&1; then
            LUA_CMD="lua"
            LUA_VERSION_CHECK=$(lua -v 2>&1 | head -1)
            echo "Found lua command: $LUA_VERSION_CHECK"
            if [[ ! "$LUA_VERSION_CHECK" =~ "Lua 5.3" ]]; then
              echo "âš ï¸ Warning: Expected Lua 5.3 but found: $LUA_VERSION_CHECK"
            fi
          elif command -v lua5.3 >/dev/null 2>&1; then
            LUA_CMD="lua5.3"
            echo "Found lua5.3 command"
          else
            echo "âŒ No Lua interpreter found"
            exit 1
          fi
          
          $LUA_CMD -e "
            package.path = package.path .. ';./?.lua;./ao-processes/?.lua;./parity-testing/?.lua'
            
            -- Try to load test runner, skip if not available
            local success, AdvancedTestRunner = pcall(require, 'ao-processes.tests.advanced-test-runner')
            if not success then
              print('âš ï¸ Advanced test runner not available, skipping performance tests')
              os.exit(0)
            end
            
            AdvancedTestRunner.configure({
              max_parallel_tests = 1, -- Performance tests should run sequentially
              enable_performance_tracking = true,
              generate_comprehensive_reports = true,
              report_formats = { 'text', 'json' }
            })
            
            local results = AdvancedTestRunner.runTestSuite({
              skip_unit_tests = true,
              skip_integration_tests = true,
              skip_parity_tests = true
            })
            
            if not results.overall_success then
              print('âš ï¸ Performance tests failed - check for regressions')
              os.exit(1)
            end
          "
      
      - name: Upload Performance Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: performance-test-results
          path: |
            test-results/
            *.log
          retention-days: ${{ env.ARTIFACT_RETENTION_DAYS }}

  # Job 6: Aggregate Results and Generate Reports
  aggregate-results:
    name: Aggregate Test Results
    runs-on: ubuntu-latest
    needs: [setup, unit-tests]  # Only unit tests for now
    if: always()
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          submodules: recursive # Include aolite submodule
      
      - name: Download All Artifacts
        uses: actions/download-artifact@v4
        with:
          path: ./all-test-results
      
      - name: Setup Lua
        uses: leafo/gh-actions-lua@v10
        with:
          luaVersion: ${{ needs.setup.outputs.lua-version }}
      
      - name: Aggregate Test Results
        run: |
          echo "ğŸ“Š Aggregating test results from all jobs..."
          
          # Count total artifacts and test results
          find ./all-test-results -name "*.log" -o -name "*.json" -o -name "*.html" | wc -l
          
          # Generate summary
          # Determine which lua command to use
          if command -v lua5.3 >/dev/null 2>&1; then
            LUA_CMD="lua5.3"
          elif command -v lua >/dev/null 2>&1; then
            LUA_CMD="lua"
          else
            echo "âŒ No Lua interpreter found, using shell summary instead"
            echo "=== Test Suite Execution Summary ==="
            echo "Jobs completed:"
            echo "  Unit Tests: ${{ needs.unit-tests.result }}"
            echo "  Integration Tests: ${{ needs.integration-tests.result }}"
            echo "  Parity Tests: ${{ needs.parity-tests.result }}"
            echo "  Performance Tests: ${{ needs.performance-tests.result }}"
            
            if [ "${{ needs.unit-tests.result }}" != "success" ]; then
              echo "âŒ Overall Status: FAILED"
              exit 1
            fi
            echo "âœ… Overall Status: SUCCESS"
            exit 0
          fi
          
          $LUA_CMD -e "
            print('=== Test Suite Execution Summary ===')
            print('Jobs completed:')
            print('  Unit Tests: ${{ needs.unit-tests.result }}')
            print('  Integration Tests: DISABLED (foundation phase)')
            print('  Parity Tests: DISABLED (no reference implementation)')
            print('  Performance Tests: DISABLED (premature)')
            print('')
            
            local overall_success = true
            if '${{ needs.unit-tests.result }}' ~= 'success' then overall_success = false end
            
            if overall_success then
              print('ğŸ‰ Overall Status: SUCCESS')
              print('âœ… AO Process Foundation Tests Passing!')
              print('ğŸ“‹ Next: Complete foundation before enabling integration/parity tests')
            else
              print('âŒ Overall Status: FAILED')
              print('âš ï¸ Unit test failures detected. Fix before proceeding.')
              os.exit(1)
            end
          "
      
      - name: Generate Final Report
        if: always()
        run: |
          # Create consolidated test report
          cat > test-summary.md << EOF
          # AO Process Test Suite Results
          
          **Execution Date:** $(date)
          **Branch:** ${{ github.ref }}
          **Commit:** ${{ github.sha }}
          
          ## Test Results Summary
          
          | Test Type | Status | 
          |-----------|--------|
          | Unit Tests | ${{ needs.unit-tests.result }} |
          | Integration Tests | DISABLED (foundation phase) |
          | Parity Tests | DISABLED (no reference impl) |
          | Performance Tests | DISABLED (premature) |
          
          ## Artifacts Generated
          - Unit test results and logs
          - Framework validation reports
          - Test coverage information
          
          ## Current Focus: Foundation Phase
          - Building core AO process functionality
          - Establishing handler framework
          - Creating test infrastructure
          
          ## Next Steps (When Foundation Complete)
          - Enable integration tests with real aolite setup
          - Implement parity testing against TypeScript reference
          - Add performance benchmarking
          - Set up deployment pipeline
          EOF
      
      - name: Upload Final Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: final-test-summary
          path: |
            test-summary.md
            ./all-test-results/**
          retention-days: ${{ env.ARTIFACT_RETENTION_DAYS }}
      
      - name: Update Status Check
        if: always()
        uses: actions/github-script@v7
        with:
          script: |
            const overallSuccess = '${{ needs.unit-tests.result }}' === 'success';
            
            const state = overallSuccess ? 'success' : 'failure';
            const description = overallSuccess ? 
              'AO process foundation tests passed - unit tests OK' : 
              'AO process unit tests failed - check results';
            
            await github.rest.repos.createCommitStatus({
              owner: context.repo.owner,
              repo: context.repo.repo,
              sha: context.sha,
              state: state,
              target_url: `${context.payload.repository.html_url}/actions/runs/${context.runId}`,
              description: description,
              context: 'AO Process Foundation Tests'
            });

  # Job 7: Notify on Failure (only for main branch)
  notify-failure:
    name: Notify on Test Failure
    runs-on: ubuntu-latest
    needs: [aggregate-results]
    if: failure() && github.ref == 'refs/heads/main'
    
    steps:
      - name: Create Issue for Test Failure
        uses: actions/github-script@v7
        with:
          script: |
            const title = `ğŸš¨ AO Process Test Suite Failure - ${new Date().toISOString()}`;
            const body = `
            ## Test Suite Failure Report
            
            **Branch:** ${context.ref}
            **Commit:** ${context.sha}
            **Workflow:** ${context.workflow}
            **Run:** ${context.runId}
            
            The AO Process test suite has failed on the main branch. This requires immediate attention.
            
            ### Action Required
            - [ ] Review test results in the workflow artifacts
            - [ ] Identify root cause of failures
            - [ ] Fix failing tests or code
            - [ ] Verify fixes with new test run
            
            ### Links
            - [Failed Workflow Run](${context.payload.repository.html_url}/actions/runs/${context.runId})
            - [Test Artifacts](${context.payload.repository.html_url}/actions/runs/${context.runId}#artifacts)
            
            This issue will be automatically closed when tests pass again.
            `;
            
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: title,
              body: body,
              labels: ['bug', 'testing', 'priority-high', 'ao-process']
            });