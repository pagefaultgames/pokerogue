# GitHub Actions Workflow for AO Process Testing
# Automated testing pipeline for development environment and parity validation

name: AO Process Test Suite

on:
  push:
    branches: [ main, beta, develop ]
    paths:
      - 'ao-processes/**'
      - 'parity-testing/**'
      - 'development-tools/**'
      - '.github/workflows/ao-process-tests.yml'
  pull_request:
    branches: [ main, beta ]
    paths:
      - 'ao-processes/**'
      - 'parity-testing/**'
      - 'development-tools/**'
  schedule:
    # Run tests daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_level:
        description: 'Test level to run'
        required: false
        default: 'full'
        type: choice
        options:
          - 'unit'
          - 'integration'
          - 'parity'
          - 'performance' 
          - 'full'
      skip_parity:
        description: 'Skip parity tests (faster execution)'
        required: false
        default: false
        type: boolean
      parallel_jobs:
        description: 'Number of parallel test jobs'
        required: false
        default: '4'
        type: string

env:
  LUA_VERSION: '5.4'
  NODE_VERSION: '18'
  TEST_TIMEOUT: '1800' # 30 minutes
  ARTIFACT_RETENTION_DAYS: '30'

jobs:
  # Job 1: Test Environment Setup and Validation
  setup:
    name: Setup Test Environment
    runs-on: ubuntu-latest
    outputs:
      lua-version: ${{ steps.setup-lua.outputs.lua-version }}
      test-strategy: ${{ steps.determine-strategy.outputs.strategy }}
      ao-ready: ${{ steps.check-ao-ready.outputs.ready }}
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 # Full history for trend analysis
      
      - name: Check AO Process Readiness
        id: check-ao-ready
        run: |
          echo "🔍 Checking if AO processes are ready for testing..."
          
          # Check if required directories and files exist for aolite-based testing
          if [ ! -d "ao-processes" ] || \
             [ ! -f "ao-processes/main.lua" ] || \
             [ ! -d "ao-processes/tests" ]; then
            echo "❌ Required AO process directories not found"
            echo "ready=false" >> $GITHUB_OUTPUT
            echo "::warning::AO Process tests skipped - basic structure not ready"
            exit 0
          fi
          
          echo "✅ AO processes ready for aolite testing"
          echo "ready=true" >> $GITHUB_OUTPUT
      
      - name: Setup Lua Environment
        id: setup-lua
        uses: leafo/gh-actions-lua@v10
        with:
          luaVersion: ${{ env.LUA_VERSION }}
      
      - name: Setup LuaRocks
        uses: leafo/gh-actions-luarocks@v4
        
      - name: Cache Lua Dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.luarocks
            ./lua_modules
          key: lua-deps-${{ runner.os }}-${{ env.LUA_VERSION }}-${{ hashFiles('**/rockspec') }}
          restore-keys: |
            lua-deps-${{ runner.os }}-${{ env.LUA_VERSION }}-
            lua-deps-${{ runner.os }}-
      
      - name: Install Aolite and Dependencies
        run: |
          echo "📦 Installing aolite for AO process testing..."
          
          # Clone aolite into development tools
          git clone https://github.com/perplex-labs/aolite.git development-tools/aolite
          
          # Verify aolite installation
          cd development-tools/aolite
          lua5.3 -e "
            local aolite = require('init')
            print('✅ Aolite loaded successfully')
            
            -- Test basic functionality
            local process_id = aolite.spawnProcess('return 42')
            local result = aolite.eval(process_id, 'return 42')
            if result == 42 then
              print('✅ Basic aolite functionality verified')
            else
              print('❌ Aolite functionality test failed')
              os.exit(1)
            end
            
            aolite.resetAll()
            print('✅ Aolite ready for testing')
          "
          
          cd ../..
      
      - name: Determine Test Strategy
        id: determine-strategy
        run: |
          if [ "${{ github.event_name }}" == "schedule" ]; then
            echo "strategy=comprehensive" >> $GITHUB_OUTPUT
          elif [ "${{ github.event_name }}" == "pull_request" ]; then
            echo "strategy=targeted" >> $GITHUB_OUTPUT
          else
            echo "strategy=standard" >> $GITHUB_OUTPUT
          fi
      
      - name: Validate Test Environment
        run: |
          echo "🔍 Validating aolite test environment..."
          lua5.3 -v
          echo "✅ Lua 5.3 environment ready"
          
          # Test aolite integration with our AO processes
          lua5.3 -e "
            package.path = package.path .. ';./development-tools/aolite/?.lua;./?.lua'
            
            local aolite = require('development-tools.aolite.init')
            
            -- Test spawning our main AO process
            print('Testing AO process loading...')
            local process_id = aolite.spawnProcess({
              file = 'ao-processes/main.lua',
              name = 'test-main'
            })
            
            print('Process spawned:', process_id)
            
            -- Test basic message sending
            local msg_id = aolite.send({
              process = process_id,
              data = '{\"action\": \"ping\"}',
              tags = { Action = 'Ping' }
            })
            
            -- Run scheduler to process message
            aolite.runScheduler()
            
            -- Check for responses
            local messages = aolite.getAllMsgs(process_id)
            print('Messages processed:', #messages)
            
            aolite.resetAll()
            print('✅ Aolite integration validated')
          "
          
          echo "✅ Test environment validated"

  # Job 2: Unit Tests
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    needs: setup
    if: >
      needs.setup.outputs.ao-ready == 'true' && (
      github.event.inputs.test_level == 'unit' || 
      github.event.inputs.test_level == 'full' ||
      github.event.inputs.test_level == '' ||
      github.event_name != 'workflow_dispatch')
    
    strategy:
      matrix:
        test-group: [1, 2, 3, 4]
      fail-fast: false
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
      
      - name: Setup Lua
        uses: leafo/gh-actions-lua@v10
        with:
          luaVersion: ${{ needs.setup.outputs.lua-version }}
      
      - name: Restore Dependencies Cache
        uses: actions/cache@v4
        with:
          path: |
            ~/.luarocks
            ./lua_modules
          key: lua-deps-${{ runner.os }}-${{ env.LUA_VERSION }}-${{ hashFiles('**/rockspec') }}
      
      - name: Run Unit Tests with Aolite
        id: unit-tests
        timeout-minutes: 15
        run: |
          echo "🧪 Running Unit Tests with Aolite (Group ${{ matrix.test-group }})"
          
          # Set test group environment variable
          export TEST_GROUP=${{ matrix.test-group }}
          export MAX_PARALLEL_TESTS=${{ github.event.inputs.parallel_jobs || '4' }}
          
          # Create aolite test output directory
          mkdir -p test-results/aolite-unit-tests
          
          # Run unit tests using aolite with our test runner
          cd development-tools/aolite
          lua5.3 -e "
            package.path = package.path .. ';./?.lua;../../?.lua;../../ao-processes/?.lua'
            
            local TestRunner = require('../../development-tools.aolite-config.test-runner')
            local success = TestRunner.runBasicTests()
            
            if not success then
              print('❌ Aolite unit tests failed')
              os.exit(1)
            end
            
            print('✅ All aolite unit tests passed')
          "
          cd ../..
      
      - name: Upload Unit Test Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: unit-test-results-group-${{ matrix.test-group }}
          path: |
            test-results/
            *.log
          retention-days: ${{ env.ARTIFACT_RETENTION_DAYS }}
      
      - name: Comment Test Results on PR
        if: github.event_name == 'pull_request' && (success() || failure())
        uses: actions/github-script@v7
        with:
          script: |
            const testGroup = '${{ matrix.test-group }}';
            const success = '${{ steps.unit-tests.outcome }}' === 'success';
            const emoji = success ? '✅' : '❌';
            const status = success ? 'PASSED' : 'FAILED';
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `${emoji} Unit Tests Group ${testGroup}: ${status}`
            });

  # Job 3: Integration Tests  
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: [setup, unit-tests]
    if: >
      always() && 
      needs.setup.outputs.ao-ready == 'true' &&
      needs.unit-tests.result == 'success' &&
      (github.event.inputs.test_level == 'integration' || 
       github.event.inputs.test_level == 'full' ||
       github.event.inputs.test_level == '' ||
       github.event_name != 'workflow_dispatch')
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
      
      - name: Setup Lua
        uses: leafo/gh-actions-lua@v10
        with:
          luaVersion: ${{ needs.setup.outputs.lua-version }}
      
      - name: Setup Node.js (for TypeScript comparison)
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
      
      - name: Install TypeScript Dependencies
        run: |
          if [ -f "package.json" ]; then
            npm install --only=dev
          fi
      
      - name: Run Integration Tests with Aolite
        id: integration-tests
        timeout-minutes: 20
        run: |
          echo "🔗 Running Integration Tests with Aolite"
          
          # Create test output directory
          mkdir -p test-results/aolite-integration-tests
          
          cd development-tools/aolite
          lua5.3 -e "
            package.path = package.path .. ';./?.lua;../../?.lua;../../ao-processes/?.lua'
            
            local aolite = require('init')
            
            print('🔗 Starting aolite-based integration tests...')
            
            -- Configure aolite for integration testing
            aolite.configure({
              log_level = 1,
              scheduler_mode = 'manual',
              output_capture = true
            })
            
            -- Integration Test: Multi-process communication
            print('Integration Test: Multi-process communication')
            
            -- Spawn main game process
            local main_process = aolite.spawnProcess({
              file = '../../ao-processes/main.lua',
              name = 'integration-main'
            })
            
            -- Spawn validator process  
            local validator_process = aolite.spawnProcess({
              file = '../../ao-processes/handlers/validation-handler.lua',
              name = 'integration-validator'
            })
            
            -- Test cross-process message flow
            aolite.send({
              process = main_process,
              data = '{\"action\": \"cross_process_test\"}',
              tags = { Action = 'CrossProcessTest', Target = validator_process }
            })
            
            aolite.runScheduler()
            
            -- Verify both processes handled messages
            local main_msgs = aolite.getAllMsgs(main_process)
            local validator_msgs = aolite.getAllMsgs(validator_process)
            
            print('Main process messages:', #main_msgs)
            print('Validator process messages:', #validator_msgs)
            
            -- Cleanup
            aolite.resetAll()
            
            print('✅ Integration tests completed successfully')
          "
          cd ../..
      
      - name: Upload Integration Test Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: integration-test-results
          path: |
            test-results/
            *.log
            *.html
          retention-days: ${{ env.ARTIFACT_RETENTION_DAYS }}

  # Job 4: Parity Tests
  parity-tests:
    name: Parity Validation Tests
    runs-on: ubuntu-latest
    needs: [setup, unit-tests]
    if: >
      always() && 
      needs.setup.outputs.ao-ready == 'true' &&
      needs.unit-tests.result == 'success' &&
      github.event.inputs.skip_parity != 'true' &&
      (github.event.inputs.test_level == 'parity' || 
       github.event.inputs.test_level == 'full' ||
       github.event.inputs.test_level == '' ||
       github.event_name != 'workflow_dispatch')
    
    strategy:
      matrix:
        parity-suite: ['pokemon-stats', 'battle-damage', 'type-effectiveness']
      fail-fast: false
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
      
      - name: Setup Lua
        uses: leafo/gh-actions-lua@v10
        with:
          luaVersion: ${{ needs.setup.outputs.lua-version }}
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
      
      - name: Install Dependencies
        run: |
          if [ -f "package.json" ]; then
            npm install
          fi
      
      - name: Run Parity Tests
        id: parity-tests
        timeout-minutes: 30
        run: |
          echo "⚖️ Running Parity Tests: ${{ matrix.parity-suite }}"
          
          lua -e "
            package.path = package.path .. ';./?.lua;./ao-processes/?.lua;./parity-testing/?.lua'
            local AdvancedTestRunner = require('ao-processes.tests.advanced-test-runner')
            
            AdvancedTestRunner.configure({
              max_parallel_tests = 2,
              test_timeout = 120000, -- 2 minute timeout for parity tests
              enable_parity_testing = true,
              generate_comprehensive_reports = true,
              report_formats = { 'text', 'json', 'html' }
            })
            
            local results = AdvancedTestRunner.runTestSuite({
              skip_unit_tests = true,
              skip_integration_tests = true,
              skip_performance_tests = true,
              parity_suite_filter = '${{ matrix.parity-suite }}'
            })
            
            if not results.overall_success then
              print('⚠️ Parity tests failed - this may indicate behavioral differences between TypeScript and Lua implementations')
              os.exit(1)
            end
          "
      
      - name: Generate Parity Report
        if: always()
        run: |
          echo "📊 Generating comprehensive parity report..."
          
          lua -e "
            package.path = package.path .. ';./?.lua;./parity-testing/?.lua'
            local ParityReporter = require('parity-testing.reports.parity-reporter')
            
            -- Generate mock results for demonstration
            local mock_results = {
              ['${{ matrix.parity-suite }}'] = {
                validation_count = 10,
                passed_count = 9,
                failed_count = 1,
                parity_rate = 0.9,
                critical_failures = 0,
                execution_time = 15000
              }
            }
            
            local report = ParityReporter.generateComprehensiveReport(mock_results, {
              format = 'html'
            })
            
            local file = io.open('parity-report-${{ matrix.parity-suite }}.html', 'w')
            if file then
              file:write(report)
              file:close()
              print('📄 Parity report generated: parity-report-${{ matrix.parity-suite }}.html')
            end
          "
      
      - name: Upload Parity Test Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: parity-test-results-${{ matrix.parity-suite }}
          path: |
            test-results/
            parity-report-*.html
            *.log
          retention-days: ${{ env.ARTIFACT_RETENTION_DAYS }}

  # Job 5: Performance Tests
  performance-tests:
    name: Performance & Benchmarking
    runs-on: ubuntu-latest
    needs: [setup, unit-tests]
    if: >
      always() && 
      needs.setup.outputs.ao-ready == 'true' &&
      needs.unit-tests.result == 'success' &&
      (github.event.inputs.test_level == 'performance' || 
       github.event.inputs.test_level == 'full' ||
       github.event_name == 'schedule')
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
      
      - name: Setup Lua
        uses: leafo/gh-actions-lua@v10
        with:
          luaVersion: ${{ needs.setup.outputs.lua-version }}
      
      - name: Run Performance Tests
        id: performance-tests
        timeout-minutes: 25
        run: |
          echo "🏃 Running Performance Tests and Benchmarks"
          
          lua -e "
            package.path = package.path .. ';./?.lua;./ao-processes/?.lua;./parity-testing/?.lua'
            local AdvancedTestRunner = require('ao-processes.tests.advanced-test-runner')
            
            AdvancedTestRunner.configure({
              max_parallel_tests = 1, -- Performance tests should run sequentially
              enable_performance_tracking = true,
              generate_comprehensive_reports = true,
              report_formats = { 'text', 'json' }
            })
            
            local results = AdvancedTestRunner.runTestSuite({
              skip_unit_tests = true,
              skip_integration_tests = true,
              skip_parity_tests = true
            })
            
            if not results.overall_success then
              print('⚠️ Performance tests failed - check for regressions')
              os.exit(1)
            end
          "
      
      - name: Upload Performance Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: performance-test-results
          path: |
            test-results/
            *.log
          retention-days: ${{ env.ARTIFACT_RETENTION_DAYS }}

  # Job 6: Aggregate Results and Generate Reports
  aggregate-results:
    name: Aggregate Test Results
    runs-on: ubuntu-latest
    needs: [setup, unit-tests, integration-tests, parity-tests, performance-tests]
    if: always()
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
      
      - name: Download All Artifacts
        uses: actions/download-artifact@v4
        with:
          path: ./all-test-results
      
      - name: Setup Lua
        uses: leafo/gh-actions-lua@v10
        with:
          luaVersion: ${{ needs.setup.outputs.lua-version }}
      
      - name: Aggregate Test Results
        run: |
          echo "📊 Aggregating test results from all jobs..."
          
          # Count total artifacts and test results
          find ./all-test-results -name "*.log" -o -name "*.json" -o -name "*.html" | wc -l
          
          # Generate summary
          lua -e "
            print('=== Test Suite Execution Summary ===')
            print('Jobs completed:')
            print('  Unit Tests: ${{ needs.unit-tests.result }}')
            print('  Integration Tests: ${{ needs.integration-tests.result }}')
            print('  Parity Tests: ${{ needs.parity-tests.result }}')
            print('  Performance Tests: ${{ needs.performance-tests.result }}')
            print('')
            
            local overall_success = true
            if '${{ needs.unit-tests.result }}' ~= 'success' then overall_success = false end
            if '${{ needs.integration-tests.result }}' == 'failure' then overall_success = false end
            if '${{ needs.parity-tests.result }}' == 'failure' then overall_success = false end
            if '${{ needs.performance-tests.result }}' == 'failure' then overall_success = false end
            
            if overall_success then
              print('🎉 Overall Status: SUCCESS')
              print('✅ AO Process Architecture is ready for deployment!')
            else
              print('❌ Overall Status: FAILED')
              print('⚠️ Some test failures detected. Review results before deployment.')
              os.exit(1)
            end
          "
      
      - name: Generate Final Report
        if: always()
        run: |
          # Create consolidated test report
          cat > test-summary.md << EOF
          # AO Process Test Suite Results
          
          **Execution Date:** $(date)
          **Branch:** ${{ github.ref }}
          **Commit:** ${{ github.sha }}
          
          ## Test Results Summary
          
          | Test Type | Status | 
          |-----------|--------|
          | Unit Tests | ${{ needs.unit-tests.result }} |
          | Integration Tests | ${{ needs.integration-tests.result }} |
          | Parity Tests | ${{ needs.parity-tests.result }} |
          | Performance Tests | ${{ needs.performance-tests.result }} |
          
          ## Artifacts Generated
          - Unit test results and logs
          - Integration test reports
          - Parity validation reports  
          - Performance benchmarks
          - HTML reports for detailed analysis
          
          ## Next Steps
          - Review any failed tests in the artifacts
          - Check parity reports for behavioral differences
          - Monitor performance regressions
          - Update documentation if needed
          EOF
      
      - name: Upload Final Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: final-test-summary
          path: |
            test-summary.md
            ./all-test-results/**
          retention-days: ${{ env.ARTIFACT_RETENTION_DAYS }}
      
      - name: Update Status Check
        if: always()
        uses: actions/github-script@v7
        with:
          script: |
            const overallSuccess = '${{ needs.unit-tests.result }}' === 'success' && 
                                 '${{ needs.integration-tests.result }}' !== 'failure' &&
                                 '${{ needs.parity-tests.result }}' !== 'failure' &&
                                 '${{ needs.performance-tests.result }}' !== 'failure';
            
            const state = overallSuccess ? 'success' : 'failure';
            const description = overallSuccess ? 
              'All AO process tests passed successfully' : 
              'Some AO process tests failed - check results';
            
            await github.rest.repos.createCommitStatus({
              owner: context.repo.owner,
              repo: context.repo.repo,
              sha: context.sha,
              state: state,
              target_url: `${context.payload.repository.html_url}/actions/runs/${context.runId}`,
              description: description,
              context: 'AO Process Test Suite'
            });

  # Job 7: Notify on Failure (only for main branch)
  notify-failure:
    name: Notify on Test Failure
    runs-on: ubuntu-latest
    needs: [aggregate-results]
    if: failure() && github.ref == 'refs/heads/main'
    
    steps:
      - name: Create Issue for Test Failure
        uses: actions/github-script@v7
        with:
          script: |
            const title = `🚨 AO Process Test Suite Failure - ${new Date().toISOString()}`;
            const body = `
            ## Test Suite Failure Report
            
            **Branch:** ${context.ref}
            **Commit:** ${context.sha}
            **Workflow:** ${context.workflow}
            **Run:** ${context.runId}
            
            The AO Process test suite has failed on the main branch. This requires immediate attention.
            
            ### Action Required
            - [ ] Review test results in the workflow artifacts
            - [ ] Identify root cause of failures
            - [ ] Fix failing tests or code
            - [ ] Verify fixes with new test run
            
            ### Links
            - [Failed Workflow Run](${context.payload.repository.html_url}/actions/runs/${context.runId})
            - [Test Artifacts](${context.payload.repository.html_url}/actions/runs/${context.runId}#artifacts)
            
            This issue will be automatically closed when tests pass again.
            `;
            
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: title,
              body: body,
              labels: ['bug', 'testing', 'priority-high', 'ao-process']
            });